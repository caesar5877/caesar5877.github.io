### 热门YouTube视频系统设计概览

热门YouTube视频系统旨在为用户提供**发现有趣视频**的功能，通过展示全球范围内的热门视频列表来提升用户体验。这个系统需要能够根据不同的时间窗口（如每日、最近一小时或实时）生成热门视频榜单。

#### 1. 需求收集 (Gather Requirements)

*   **功能性需求 (Functional Requirements)**:
    *   系统需展示一个**全球热门视频列表**，可在网页或移动客户端显示。
    *   热门榜单应支持**不同时间窗口**，例如每日、最近一小时和实时榜单。
    *   视频的热门程度目前假定由**观看次数**决定（即观看次数最多的视频获胜）。
    *   系统需记录用户观看视频的事件。
    *   API应能获取特定时间范围内的Top K视频。
*   **非功能性需求 (Non-Functional Requirements)**:
    *   **规模 (Scale)**：系统需支持**5亿日活跃用户 (DAU)**，分布在全球各地。视频分布呈现长尾效应，少数热门视频占据大部分流量，总视频数量约100亿。
    *   **准确性 (Accuracy)**：榜单结果应**尽可能准确**，且未来可能与财务挂钩，因此准确性非常重要，但可接受略微不准确。
    *   **新鲜度 (Freshness)**：希望能够提供最新数据，但可接受**几分钟的延迟**。对于视图计数，目前允许一到两小时的延迟。
    *   **延迟 (Latency)**：获取热门视频的查询延迟应尽可能低。页面加载速度快，例如p99在300毫秒左右。
    *   **持久性 (Durability)**：视频的观看次数数据**非常重要**，需要持久化存储。
    *   **可用性 (Availability)**：指标收集服务（如观看计数服务）的可用性**极其重要**，不能阻止页面加载。
    *   **一致性 (Consistency)**：对于视频观看次数，由于用户之间不太可能实时交流确切数字，且不存在因果顺序，因此一致性并非首要关注点。

#### 2. API 定义 (Define API)

系统需要定义以下核心API：

*   `watch_video(user_id, video_id)` → `status`：当用户观看视频时调用此API。目前不考虑去重等复杂情况。
*   `get_top_videos(k, start_time, end_time)` → `[video_info]`：获取特定时间范围内（K最大100，时间粒度为分钟）的Top K视频列表。`video_info`包含`video_id`及用于显示的相关元数据。
*   `view_video(video_id)` → `count`：加载视频页面并同时增加观看计数。此API简化为获取计数的API。

#### 3. 高层架构图 (High-Level Diagram)

系统的核心组件包括：

*   **事件队列 (Event Queue)**：用于接收`watch_video`事件，处理高写入吞吐量。
*   **聚合器服务 (Aggregator Service)**：从事件队列中拉取事件，并将其汇总到度量存储中。
*   **指标存储 (Metrics Storage)**：持久化存储聚合后的观看次数数据。
*   **YouTube服务 (YouTube Service)**：处理`watch_video`请求并向事件队列发送事件。
*   **指标服务 (Metrics Service)**：一个轻量级服务，负责从指标存储中读取数据，以响应`get_top_videos`请求。

架构设计理念是异步处理观看事件，通过队列吸收流量高峰，然后由聚合服务将事件汇总为分钟级别的指标，并存储到数据库中。读取请求则直接从指标存储中获取数据。

#### 4. 模式和数据结构 (Schema and Data Structures)

*   **事件队列 (Event Queue)**：存储`video_id`和`timestamp`，代表用户观看视频的事件。
*   **指标存储 (Metrics Storage)**：
    *   **分钟指标表 (Minute Metrics Table)**：存储`Minute` (分钟时间戳), `Video ID`, `Count`。用于分钟粒度的汇总数据。
    *   **视图存储 (View Storage)**：存储`Video Id`, `Count`，用于存储每个视频的总观看次数。

#### 5. 端到端流程总结 (Summarize End-to-End Flow)

*   **观看视频 (`watch_video`) 流程**: 当用户观看视频时，`YouTube Service` 调用 `watch_video` API，生成一个包含 `video_id` 和 `timestamp` 的事件，并将其发送到**事件队列**。**聚合器服务**从队列中定期拉取事件批次，将它们汇总为分钟粒度的计数，并存储到**指标存储**（分钟指标表）。
*   **获取热门视频 (`get_top_videos`) 流程**: 用户通过 `get_top_videos` API 请求热门视频。`Metrics Service` 作为中间层，从**指标存储**（分钟指标表）中查询指定时间范围内的热门视频，然后返回给客户端。
*   **视图计数 (`view_video`) 流程**: 当用户访问视频页面时，会调用 `view_video` API，触发一个事件发送到**视图队列**。**视图指标处理器**处理这些事件并更新**视图存储**中的总计数。同时，`Video Service` 从**视图存储**中获取最新计数并返回给用户。

#### 6. 深入探讨 (Deep Dives)

以下是系统设计中识别出的关键痛点及其解决方案和权衡：

1.  **客户端与服务器时间戳生成 (Client versus Server Timestamp)**
    *   **问题**：时间戳是事件聚合的关键。如果由客户端生成，可能存在**设备时钟偏差和恶意篡改**的风险；如果由服务器生成，则可能因**网络延迟或离线观看**导致时间戳不准确。
    *   **解决方案与权衡**：
        *   **方案1：客户端生成时间戳**：优点是事件时间更准确，离线也能捕获。缺点是可能因时钟偏差和恶意篡改导致不准确。
        *   **方案2：YouTube服务生成时间戳**：优点是服务器时钟更一致，但可能因网络延迟或离线观看导致时间戳不准确。
    *   **结论**：由于不直接支持离线使用场景，且不预期指标收集严重延迟，**选择方案2（服务器生成时间戳）**。未来可考虑捕获客户端和服务器时间戳，通过差值近似客户端时钟偏差，并监控异常时间戳来应对恶意行为。

2.  **聚合服务数据结构 (Aggregation Service Data Structure)**
    *   **问题**：在每分钟聚合完成后，需要从大量的视频中高效地找出Top K视频。如果直接对列表排序，效率会很低 (`N log N`)。
    *   **解决方案与权衡**：
        *   **方案1：对列表排序**：实现简单，但效率低 (`N log N`)。
        *   **方案2：使用大小为K的最小堆 (Min Heap of Size K)**：效率更高 (`K log K`)，尤其当K远小于N时。
    *   **结论**：鉴于K通常远小于N（视频总数），**选择方案2（使用最小堆）**以提高效率。

3.  **聚合服务扩容深入探讨 (Aggregation Service Scaling Deep Dive)**
    *   **问题**：假设每分钟有1亿个独立视频观看，缓冲20分钟，保守估计需要60GB内存。如果可用机器内存有限（例如16GB），且视频数量超过1亿，内存将成为瓶颈。此外，长尾分布导致部分热门视频可能成为**热点**。
    *   **解决方案与权衡**：
        *   **方案1：分片到多个聚合服务**：按`video_id`通过**一致性哈希**分片，每个分片独立维护计数表。优点是能扩展计算能力。缺点是增加了协调复杂性，且热门视频仍可能导致热点。
        *   **方案2：随机插入到聚合服务**：事件随机分发到聚合器，聚合后由协调器合并结果。优点是**均匀分布负载，有效避免热点**。缺点是合并所有节点计数时可能需要“散列-收集”（scatter-gather）操作。
        *   **方案3：使用概率数据结构 (Probabilistic Data Structure)**：例如**Count-Min Sketch**，通过牺牲部分准确性来显著降低内存占用。缺点是结果为近似值。
        *   **方案4：引入批处理管道 (Batch Pipeline)**：结合**Lambda架构**，用批处理管道进行较慢但更准确的最终聚合，以弥补流处理的潜在不准确性。缺点是维护两个系统的复杂性。
    *   **结论**：为应对内存限制和热点问题，同时兼顾速度，**选择方案2（随机插入到聚合服务）**，因为它能有效避免热点并提高吞吐量。若需要更高准确性，可考虑**方案3（概率数据结构）**，甚至结合**方案4（Lambda架构）**。

4.  **指标存储深入探讨 (Metrics Storage Deep Dive)**
    *   **问题**：目前只存储分钟粒度的Top K视频。如果需要获取小时或每日的聚合数据，直接从分钟数据聚合会非常低效且**不准确**，因为分钟Top K不包含所有视频的完整计数。
    *   **解决方案与权衡**：
        *   **方案1：直接聚合分钟间隔**：简单，但**数据不准确**，因为分钟Top K不是全量数据。
        *   **方案2：增加缓冲区 (Include Some Buffer)**：存储Top K + 额外的视频，减少数据丢失的可能性。缺点是存储和处理数据量增加，且长时间窗口仍可能丢失数据。
        *   **方案3：在流处理中保留小时数据**：流处理同时维护小时粒度的数据。优点是准确且及时。缺点是**内存需求大幅增加**。
        *   **方案4：小时批处理作业 (Hourly Batch Job)**：通过批处理作业计算小时数据。优点是准确性高。缺点是批处理作业通常延迟更高，且可能不稳定。
    *   **结论**：为实现小时/每日数据的准确性，**优先考虑方案3（在流处理中保留小时数据）**，因为它在保证准确性的同时能及时获取结果。如果内存仍是瓶颈，可结合**Count-Min Sketch**。同时，可考虑**方案4（小时批处理作业）**作为补充，以覆盖流处理的潜在不准确性。

5.  **聚合服务故障 (Aggregation Service Failures)**
    *   **问题**：如果聚合服务（流处理组件）发生故障，内存中的聚合数据将丢失，需要从头开始重建，这将导致大量未处理数据积压。
    *   **解决方案与权衡**：
        *   **方案1：本地磁盘检查点 (Checkpoint on Local Disk)**：定期将聚合数据快照写入本地磁盘。优点是检查点和恢复速度快。缺点是如果整个实例故障，本地磁盘数据也会丢失。
        *   **方案2：分布式存储检查点 (Checkpoint on a Distributed Store)**：将检查点写入分布式存储。优点是可扩展且降低了相关性故障的风险。缺点是网络开销会减慢检查点和重建速度。
        *   **方案3：本地磁盘与分布式存储结合**：更频繁地写入本地磁盘进行进程内故障恢复，同时不频繁地将数据转储到分布式存储以应对实例级故障。缺点是异步备份可能导致数据丢失或延迟。
    *   **结论**：**选择方案3（本地磁盘与分布式存储结合）**，在恢复速度和持久性之间取得平衡。可以调整检查点频率：更频繁的检查点会缩短恢复时间但降低处理性能。

6.  **晚到事件处理 (Late Events)**
    *   **问题**：在流处理中，事件可能会因网络中断或设备离线等原因**延迟到达或乱序到达**。系统需要机制来处理这些晚到事件。
    *   **解决方案与权衡**：
        *   **水印延迟 (Watermark Delay)**：设置一个水印（watermark），指示所有晚于此时间戳的事件应被特殊处理（如丢弃或单独处理）。
            *   **方案1：更长的水印延迟 (Longer Watermark Delay)**：优点是准确性更高，能捕获更多晚到事件。缺点是流处理需要持有更多数据，占用更多内存。
            *   **方案2：更短的水印延迟 (Shorter Watermark Delay)**：优点是更快确定结果，内存占用少。缺点是可能丢弃更多晚到事件。
    *   **结论**：由于数据准确性有一定的灵活度，且假设内部网络更可靠（使用服务器生成时间戳），**倾向于选择方案2（更短的水印延迟）**以减少内存占用和加速结果确定。

7.  **水印后事件处理 (Post Watermark Processing)**
    *   **问题**：当事件晚于水印到达时，系统如何处理？是丢弃还是尝试更新现有数据？
    *   **解决方案与权衡**：
        *   **方案1：丢弃 (Discard It)**：优点是实现简单。缺点是会导致数据丢失，降低准确性。
        *   **方案2：修改现有指标存储 (Modify the Existing Metrics Storage)**：将晚到事件发送到另一个管道进行修改处理。优点是能提高准确性。缺点是会**显著增加复杂性**。
    *   **结论**：鉴于准确性有一定的灵活度，且已有批处理管道作为“慢路径”最终会解决数据不准确问题，**选择方案1（丢弃）**，避免引入修改管道的复杂性。

8.  **读取扩容 (Scaling for Read - Total View Count)**
    *   **问题**：假设使用CRDT数据结构且每个节点将计数存储在磁盘上，读取吞吐量可能高达50万 QPS，需要进行扩容。
    *   **解决方案与权衡**：
        *   **方案1：增加CRDT节点**：分片数据库以增加可读节点。缺点是CRDT节点越多，**扇出（fan-out）开销越大**，每个机器需要向其他机器推送数据。
        *   **方案2：复制到读副本 (Replicate to Read Replicas)**：为每个CRDT节点创建读副本，以增加读取吞吐量。优点是冗余和读取扩容。缺点是如果使用异步复制，读副本可能**最终一致**（数据滞后），但鉴于CRDT本身就是最终一致的，这是可接受的。
        *   **方案3：读穿透缓存 (Read-Through Cache)**：使用读穿透缓存，缓存未命中时从后端获取数据。优点是可以通过缓存副本扩展读取。缺点是**缓存失效非常困难**，因为计数不断更新，需要频繁失效。
        *   **方案4：定期更新缓存 (Periodic Update to Cache)**：通过定期作业更新缓存中的最新计数。优点是可以很好地扩展缓存数量。缺点是会引入延迟，但可调整以适应新鲜度需求。
    *   **结论**：鉴于对最终一致性的可接受度，**选择方案4（定期更新缓存）**来满足读取扩容需求，并通过扩容缓存集群来实现。方案2用于冗余而非读取扩容。
